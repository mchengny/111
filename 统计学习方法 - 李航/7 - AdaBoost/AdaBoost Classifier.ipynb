{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Define the base classifier\n",
    "- Potential Bug:  \n",
    "\n",
    "After splitting the dataset, sometimes the left and right label may have the same result because of the same frequency of labels on each side, this will lead to a wrong choice of splitted feature and value.  \n",
    "\n",
    "Finally, the iterations of weight will go into the same loop, and the error of model will not be convergent. That is what we dont hope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use decision tree stump as base learner of AdaBoost model\n",
    "class BaseClassifier():\n",
    "    \n",
    "    def __init__(self,X_train, Y_train, weights):\n",
    "        \"\"\"\n",
    "        Initialize the parameters of decision tree stump, and create the decision tree stump as basic clasfier\n",
    "        \n",
    "        Arguments:\n",
    "        e -- the error distribution of this model\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        weights -- the weights of importance between each data point\n",
    "        left -- label corresponding to the left leaf\n",
    "        right -- label corresponding to the right leaf\n",
    "        signs -- the signs of y_hat*y_true\n",
    "       \n",
    "        \"\"\"\n",
    "        # Initialize the parameters\n",
    "        self.e = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_dim = None\n",
    "        self.split_value = None\n",
    "        \n",
    "        # Fit  model with training data under the given weights\n",
    "        self.fit(X_train, Y_train, weights)\n",
    "        # Get the label of left leaf and right leaf\n",
    "        index_left = np.where(X_train[:,self.split_dim]<=self.split_value)\n",
    "        self.left = self.compute_label(Y_train[index_left])\n",
    "        index_right = np.where(X_train[:,self.split_dim]>self.split_value)\n",
    "        self.right = self.compute_label(Y_train[index_right])\n",
    "        # Get the signs of y_hat and y_true     \n",
    "        self.signs = self.pred_sign(X_train, Y_train)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, Y_train, weights):\n",
    "        \"\"\"\n",
    "        Fit the decision tree stump under given weights\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        weights -- the weights of importance between each data point\n",
    "        \n",
    "        Returns:\n",
    "        split_dim -- the dimension which dataset will be splitted along\n",
    "        split_value -- the value which dataset will be splitted dependent on\n",
    "        \n",
    "        \"\"\"\n",
    "        # Get the shape of feature space\n",
    "        num_x = X_train.shape[0]\n",
    "        dim_x = X_train.shape[1]\n",
    "        \n",
    "        # Find the optimal split-dimension and split-value\n",
    "        self.e = np.inf\n",
    "        for dim in range(dim_x):\n",
    "            temp_slice = X_train[:,dim]\n",
    "            for value in np.unique(temp_slice):\n",
    "                # Compute current y_hats and error under this status \n",
    "                y_hat = self.compute_y_hat(X_train, Y_train, dim, value)\n",
    "                error = self.compute_error(y_hat, Y_train, weights)\n",
    "                if error < self.e:\n",
    "                    self.e = error\n",
    "                    self.split_dim = dim\n",
    "                    self.split_value = value\n",
    "        \n",
    "        return  None\n",
    "    \n",
    "    \n",
    "    def compute_y_hat(self, X_train, Y_train, split_dim, split_value):\n",
    "        \"\"\"\n",
    "        Compute the prediction of training data under splitting\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        split_dim -- the dimension which dataset will be splitted along\n",
    "        split_value -- the value which dataset will be splitted dependent on\n",
    "        \n",
    "        Return:\n",
    "        y_hats -- the predictions of training data under current model\n",
    "        \"\"\"\n",
    "        # Get the corresponding label of each leaf\n",
    "        index_left = np.where(X_train[:,split_dim]<=split_value)\n",
    "        left_label = self.compute_label(Y_train[index_left])\n",
    "        index_right = np.where(X_train[:,split_dim]>split_value)\n",
    "        right_label = self.compute_label(Y_train[index_right])\n",
    "        # Calculate the final y_hats of training data\n",
    "        y_hats = []\n",
    "        for data in X_train:\n",
    "            if data[split_dim] <= split_value:\n",
    "                y_hats.append(left_label)\n",
    "            else:\n",
    "                y_hats.append(right_label)\n",
    "        \n",
    "        return y_hats\n",
    "    \n",
    "    \n",
    "    def compute_error(self, y_hat, y_true, weights):\n",
    "        \"\"\"\n",
    "        Compute the error between y_hat and y_true under given weights\n",
    "        \n",
    "        Arguments:\n",
    "        y_hat -- the predictions of training data \n",
    "        y_true -- the true values of training data\n",
    "        weights -- the weights of importance between each data point\n",
    "        \n",
    "        Return:\n",
    "        error -- the total error under this classification model\n",
    "        \n",
    "        \"\"\"\n",
    "        error = 0\n",
    "        for i in range(len(y_hat)):\n",
    "            if y_hat[i] != y_true[i]:\n",
    "                error += weights[i]\n",
    "                \n",
    "        return error\n",
    "    \n",
    "\n",
    "    def compute_label(self, data_list):\n",
    "        \"\"\"\n",
    "        Compute the prediction of training data under splitting\n",
    "        \n",
    "        Argument:\n",
    "        data_list -- a list of labels \n",
    "        \n",
    "        Return:\n",
    "        the label of this group\n",
    "        \n",
    "        \"\"\"\n",
    "        d= {}\n",
    "        # Calculate the frequency of different labels\n",
    "        for n in data_list:\n",
    "            if n in d:\n",
    "                d[n] += 1\n",
    "            else:\n",
    "                d[n] = 1\n",
    "                \n",
    "        # Return the label which has max frequency\n",
    "        if len(d) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return max(d, key=d.get)\n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the result of given x\n",
    "        \n",
    "        Argument:\n",
    "        x -- input feature vector with the shape of [1, n_dimensions]\n",
    "        \n",
    "        Return:\n",
    "        the prediction of given x\n",
    "        \n",
    "        \"\"\"\n",
    "        if x[self.split_dim] <= self.split_value:\n",
    "            return self.left\n",
    "        else:\n",
    "            return self.right\n",
    "    \n",
    "    \n",
    "    def pred_sign(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        Compute the signs of y_hat*y_true\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        \n",
    "        Return:\n",
    "        the list consist of signs of y_hat*y_true\n",
    "        \n",
    "        \"\"\"\n",
    "        signs = []\n",
    "        for i in range(len(X_train)):\n",
    "            pred = self.predict(X_train[i])\n",
    "            if pred == Y_train[i]:\n",
    "                signs.append(1)\n",
    "            else:\n",
    "                signs.append(-1)\n",
    "        \n",
    "        return np.array(signs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Define the AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting Algorithm based on Decision Tree Stump\n",
    "class AdaBoost():\n",
    "    \n",
    "    def __init__(self, n_estimators=3, min_error=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the parameters\n",
    "        \n",
    "        Arguments:\n",
    "        alpha -- a vector consist of each coefficient of each child-model\n",
    "        estimators -- a list of child estimators in ensemble leaning model\n",
    "        n_estimators -- the number of estimators in model\n",
    "        min_error -- stop condition for the training\n",
    "        \n",
    "        \"\"\"     \n",
    "        self.alpha = np.array([])\n",
    "        self.estimators = []\n",
    "        self.min_error = min_error\n",
    "        self.n_estimators = n_estimators\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit the model with training data\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        \n",
    "        \"\"\"  \n",
    "        # Get the shape of feature space\n",
    "        num_x = X_train.shape[0]\n",
    "        dim_x = X_train.shape[1]\n",
    "        \n",
    "        # Set the initial weight as uniform distribution\n",
    "        weight = np.ones(num_x) / num_x\n",
    "        # Generate model step by step\n",
    "        for i in range(self.n_estimators):\n",
    "            # Create current optimal model under given weight\n",
    "            model = BaseClassifier(X_train, Y_train, weight)\n",
    "            # Calculate the error of current model\n",
    "            e = model.e\n",
    "             \n",
    "            # Whether need to stop training\n",
    "            if e <=self.min_error:\n",
    "                self.estimators = [model]\n",
    "                self.alpha = np.array([1])\n",
    "                break\n",
    "            \n",
    "            # Calculate the alpha-coefficient of current model\n",
    "            a = 0.5 * np.log((1-e)/e)\n",
    "            # Get the sign of each prediction of current model\n",
    "            s = model.signs\n",
    "            \n",
    "            # Update weight and normalize\n",
    "            weight = weight * np.exp(-1*a*s)\n",
    "            weight = weight / np.sum(weight)\n",
    "            \n",
    "            # Add alpha and estimators into model sequence\n",
    "            self.alpha = np.append(self.alpha, a)\n",
    "            self.estimators.append(model)\n",
    "            \n",
    "            # Whether end training\n",
    "            if self.accuracy(X_train, Y_train) <=self.min_error:\n",
    "                break\n",
    "            \n",
    "            if verbose == True:\n",
    "                print('Step',i)\n",
    "                print('---------------------')\n",
    "                print('error:', e)\n",
    "                print('alpha:', a)\n",
    "                print('weight:', weight)\n",
    "                print('\\n')\n",
    "                \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Get the prediciton of given x\n",
    "        \n",
    "        Argument:\n",
    "        x -- input feature vector \n",
    "        \n",
    "        Return:\n",
    "        the prediction of AdaBoost model\n",
    "        \"\"\"  \n",
    "        # Set the initial frequency dict \n",
    "        d = {}\n",
    "        # Calculate the predction of each model and summarize them\n",
    "        for i in range(len(self.estimators)):\n",
    "            model = self.estimators[i]\n",
    "            pred = model.predict(x)\n",
    "            if pred in d:\n",
    "                d[pred] += self.alpha[i]\n",
    "            else:\n",
    "                d[pred] = self.alpha[i]\n",
    "                \n",
    "        # Find the key which has the max value\n",
    "        return max(d, key=d.get)\n",
    "    \n",
    "    \n",
    "    def accuracy(self, X_train, Y_train):\n",
    "        \"\"\"\n",
    "        Get the accuracy of predictions\n",
    "        \n",
    "        Arguments:\n",
    "        X_train -- training dataset with the shape of [n_samples, n_features]\n",
    "        Y_train -- corresponding labels of training dataset, with the length of n_samples\n",
    "        \n",
    "        Return:\n",
    "        accuracy -- the accuracy of classification\n",
    "        \n",
    "        \"\"\"  \n",
    "        # Get the predictions\n",
    "        preds = []\n",
    "        for i in X_train:\n",
    "            pred = self.predict(i)\n",
    "            preds.append(pred)\n",
    "        \n",
    "        # Calculate the accuracy\n",
    "        accuracy = 0\n",
    "        num = len(preds)\n",
    "        for i in range(num):\n",
    "            if preds[i] == Y_train[i]:\n",
    "                accuracy += 1 / num\n",
    "        \n",
    "        return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*P40 例 8.1*  \n",
    "给定如下表所示训练数据集。假设弱分类器由x<v或x>v产生，其阈值v使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习一个强分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 序号|  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 | 10 |\n",
    "| :--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "|  x  |  0 |  1 |  2 |  3 |  4 |  5 |  6 |  7 |  8 |  9 |\n",
    "|  y  | -1 |  1 |  1 | -1 | -1 | -1 |  1 |  1 |  1 | -1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate artificial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array([[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]])\n",
    "y = np.array([-1, 1, 1,-1,-1,-1, 1, 1, 1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating and training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "---------------------\n",
      "error: 0.3\n",
      "alpha: 0.423648930194\n",
      "weight: [ 0.07142857  0.16666667  0.16666667  0.07142857  0.07142857  0.07142857\n",
      "  0.07142857  0.07142857  0.07142857  0.16666667]\n",
      "\n",
      "\n",
      "Step 1\n",
      "---------------------\n",
      "error: 0.285714285714\n",
      "alpha: 0.458145365937\n",
      "weight: [ 0.125       0.11666667  0.11666667  0.05        0.05        0.05        0.125\n",
      "  0.125       0.125       0.11666667]\n",
      "\n",
      "\n",
      "Step 2\n",
      "---------------------\n",
      "error: 0.266666666667\n",
      "alpha: 0.505800455839\n",
      "weight: [ 0.08522727  0.07954545  0.07954545  0.09375     0.09375     0.09375\n",
      "  0.08522727  0.08522727  0.08522727  0.21875   ]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoost()\n",
    "model.fit(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [-1, 1, 1, -1, -1, -1, 1, 1, 1, 1]\n",
      "Accuracy: 0.8999999999999999\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for i in range(len(x)):\n",
    "    pred = model.predict(x[i])\n",
    "    preds.append(pred)\n",
    "\n",
    "print('Predictions:', preds)\n",
    "print('Accuracy:', model.accuracy(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### improvement\n",
    "- increasing the number of estimators can enhance the performance of AdaBoost obviously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "improved_model = AdaBoost(n_estimators=100)\n",
    "improved_model.fit(x, y)\n",
    "\n",
    "print('Accuracy:', improved_model.accuracy(x, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
